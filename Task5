PyTorch实现L1，L2正则化以及Dropout
1.了解知道Dropout原理
2.用代码实现正则化(L1、L2、Dropout）
3.Dropout的numpy实现
4.PyTorch中实现dropout



    Dropout（随机失活）是防止过拟合的一种方法（过拟合overfitting指：模型在训练数据上损失函数较小，预测准确率较高；
但是在测试数据上损失函数比较大，预测准确率较低。） 
    训练神经网络模型时，如果训练样本较少，为了防止模型过拟合，Dropout可以作为一种优化方法。
    Dropout是指在神经网络的每次训练中以一个参数p为概率，使部分隐层部分神经元失活，以此来解决过拟合问题，
效果可以当作用多个不同的神经网络模型在同一训练集上进行训练，最后集成求平均。
（我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征）
    Dropout还可以消除某些神经元之间的联系，增强模型的鲁棒性。
    Dropout具体工作流程：先随机删除一半（概率p=1/2）的神经元（保留），然后训练剩下的一半，训练完更新参数，把刚才删除的神经元恢复，再随机删除
一半的神经元，还是刚才的步骤。。
    正则化就是把结构化风险最小化的过程，也可以理解为防止过拟合，也就是让w项量中项的个数最小，也是w零范数最小
    L2正则化是在原来的损失函数上加上模型权重参数的平方和（正则化项）。
    L1正则化则是在原来的损失函数上加上权重参数的绝对值之和（正则化项）。
