{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 1 1 1 0 1 0]\n",
      "[1. 0. 0. 4. 5. 6. 7. 0. 9. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.6666666,  0.       ,  0.       ,  6.6666665,  8.333333 ,\n",
       "       10.       , 11.666666 ,  0.       , 14.999999 ,  0.       ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "# dropout函数的实现，函数中，x是本层网络的激活值。Level就是dropout就是每个神经元要被丢弃的概率。\n",
    "def dropout(x, level):\n",
    "    if level < 0. or level >= 1: #level是概率值，必须在0~1之间\n",
    "        raise ValueError('Dropout level must be in interval [0, 1[.')\n",
    "    retain_prob = 1. - level\n",
    " \n",
    "    # 我们通过binomial函数，生成与x一样的维数向量。binomial函数就像抛硬币一样，我们可以把每个神经元当做抛硬币一样\n",
    "    # 硬币 正面的概率为p，n表示每个神经元试验的次数\n",
    "    # 因为我们每个神经元只需要抛一次就可以了所以n=1，size参数是我们有多少个硬币。\n",
    "    random_tensor = np.random.binomial(n=1, p=retain_prob, size=x.shape) #即将生成一个0、1分布的向量，0表示这个神经元被屏蔽，不工作了，也就是dropout了\n",
    "    print(random_tensor)\n",
    " \n",
    "    x *= random_tensor\n",
    "    print(x)\n",
    "    x /= retain_prob\n",
    " \n",
    "    return x\n",
    " \n",
    "#对dropout的测试，大家可以跑一下上面的函数，了解一个输入x向量，经过dropout的结果  \n",
    "x=np.asarray([1,2,3,4,5,6,7,8,9,10],dtype=np.float32)\n",
    "dropout(x,0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch.manual_seed(1)    # reproducible\n",
    "\n",
    "N_SAMPLES = 20\n",
    "N_HIDDEN = 300\n",
    "\n",
    "# training data\n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, N_SAMPLES), 1)\n",
    "y = x + 0.3*torch.normal(torch.zeros(N_SAMPLES, 1), torch.ones(N_SAMPLES, 1))\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "# test data\n",
    "test_x = torch.unsqueeze(torch.linspace(-1, 1, N_SAMPLES), 1)\n",
    "test_y = test_x + 0.3*torch.normal(torch.zeros(N_SAMPLES, 1), torch.ones(N_SAMPLES, 1))\n",
    "test_x, test_y = Variable(test_x, volatile=True), Variable(test_y, volatile=True)\n",
    "\n",
    "# show data\n",
    "'''\n",
    "plt.scatter(x.data.numpy(), y.data.numpy(), c='magenta', s=50, alpha=0.5, label='train')\n",
    "plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c='cyan', s=50, alpha=0.5, label='test')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim((-2.5, 2.5))\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "net_overfitting = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, N_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN, 1),\n",
    ")\n",
    "\n",
    "net_dropped = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, N_HIDDEN),\n",
    "    torch.nn.Dropout(0.5),  # drop 50% of the neuron\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "    torch.nn.Dropout(0.5),  # drop 50% of the neuron\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN, 1),\n",
    ")\n",
    "\n",
    "print(net_overfitting)  # net architecture\n",
    "print(net_dropped)\n",
    "\n",
    "optimizer_ofit = torch.optim.Adam(net_overfitting.parameters(), lr=0.01)\n",
    "optimizer_drop = torch.optim.Adam(net_dropped.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(500):\n",
    "    pred_ofit = net_overfitting(x)\n",
    "    pred_drop = net_dropped(x)\n",
    "\n",
    "    loss_ofit = loss_func(pred_ofit, y)\n",
    "    loss_drop = loss_func(pred_drop, y)\n",
    "\n",
    "    optimizer_ofit.zero_grad()\n",
    "    optimizer_drop.zero_grad()\n",
    "    loss_ofit.backward()\n",
    "    loss_drop.backward()\n",
    "    optimizer_ofit.step()\n",
    "    optimizer_drop.step()\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        # change to eval mode in order to fix drop out effect\n",
    "        net_overfitting.eval()\n",
    "        net_dropped.eval()  # parameters for dropout differ from train mode\n",
    "\n",
    "        # plotting\n",
    "        plt.cla()\n",
    "        test_pred_ofit = net_overfitting(test_x)\n",
    "        test_pred_drop = net_dropped(test_x)\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy(), c='magenta', s=50, alpha=0.3, label='train')\n",
    "        plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c='cyan', s=50, alpha=0.3, label='test')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_ofit.data.numpy(), 'r-', lw=3, label='overfitting')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_drop.data.numpy(), 'b--', lw=3, label='dropout(50%)')\n",
    "        plt.text(0, -1.2, 'overfitting loss=%.4f' % loss_func(test_pred_ofit, test_y).data[0], fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.text(0, -1.5, 'dropout loss=%.4f' % loss_func(test_pred_drop, test_y).data[0], fontdict={'size': 20, 'color': 'blue'})\n",
    "        plt.legend(loc='upper left'); plt.ylim((-2.5, 2.5));plt.pause(0.1)\n",
    "\n",
    "        # change back to train mode\n",
    "        net_overfitting.train()\n",
    "        net_dropped.train()\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
