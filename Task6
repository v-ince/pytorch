PyTorch理解更多神经网络优化方法
1.了解不同优化器
2.书写优化器代码
3.Momentum
4.二维优化，随机梯度下降法进行优化实现
5.Ada自适应梯度调节法
6.RMSProp
7.Adam
8.PyTorch优化器选择

神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）。


神经网络中的优化器用来干什么
（1）优化器用来寻找函数的最小值

（2）神经网络中采用优化器来寻找损失函数（loss function）的最小值

（3）代价函数最小时即找到神经网络中最合适的权重和偏置

神经网络优化器，主要是为了优化我们的神经网络，使他在我们的训练过程中快起来，节省社交网络训练的时间。
在pytorch中提供了torch.optim方法优化我们的神经网络，torch.optim是实现各种优化算法的包。他的设计很灵活，可以扩展为自定义的优化方法。
所有的优化方法都是继承了基类optim.Optimizer。并实现了自己的优化步骤。
最常用的方法都已经支持，接口很常规，所以以后也可以很容易地集成更复杂的方法。


1、梯度下降优化法：
标准梯度下降法（GD，Grandient Descent），
随机梯度下降法（SGD，Stochastic Gradient Descent）：SGD是最基础的优化方法，普通的训练方法, 需要重复不断的把整套数据放入神经网络NN中训练,
               这样消耗的计算资源会很大.当我们使用SGD会把数据拆分后再分批不断放入 NN 中计算. 每次使用批数据, 虽然不能反映整体数据的情况,
               不过却很大程度上加速了 NN 的训练过程, 而且也不会丢失太多准确率.
批量梯度下降法（BGD，Batch Gradient Descent）：批量梯度下降每次学习都使用整个训练集
2、动量优化法：
Momentum(动量)：带momentum的梯度下降法也是一种很常用的的优化算法。这种方法因为引入了momentum量，所以能够对梯度下降法起到加速的作用。
Nesterov accelerated gradient（NAG）：不仅增加了动量项，并且在计算参数的梯度时，在损失函数中减去了动量项
3、自适应学习率优化法：
Adagrad：基于梯度的优化算法，它能够对每个参数自适应不同的学习速率，对稀疏特征，得到大的学习更新，对非稀疏特征，得到较小的学习更新，
         因此该优化算法适合处理稀疏特征数据。Adagrad是一种自适应优化方法，是自适应的为各个参数分配不同的学习率。
         这个学习率的变化，会受到梯度的大小和迭代次数的影响。梯度越大，学习率越小；梯度越小，学习率越大。缺点是训练后期，学习率过小，
         因为Adagrad累加之前所有的梯度平方作为分母。
AdaDelta：对 Adagrad 的改进，和 Adagrad 相比，就是分母的 G 换成了过去的梯度平方的衰减平均值。
          Adadelta是Adagrad的改进。Adadelta分母中采用距离当前时间点比较近的累计项，这可以避免在训练后期，学习率过小。
RMSprop：RMSprop 是 Geoff Hinton 提出的一种自适应学习率方法。RMSprop 和 Adadelta 都是为了解决 Adagrad 学习率急剧下降问题的。
         实现RMSprop优化方法（Hinton提出），RMS是均方根（root meam square）的意思。RMSprop和Adadelta一样，也是对Adagrad的一种改进。
         RMSprop采用均方根作为分母，可缓解Adagrad学习率下降较快的问题。并且引入均方根，可以减少摆动
Adam：Adam方法就是根据上述思想而提出的，对于每个参数，其不仅仅有自己的学习率，还有自己的Momentum量，
      这样在训练的过程中，每个参数的更新都更加具有独立性，提升了模型训练速度和训练的稳定性。


